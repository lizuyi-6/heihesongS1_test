# å¼€å‘ä»»åŠ¡åˆ†è§£ä¹¦

**é¡¹ç›®**ï¼šç›´æ’­é—´"åˆ‡ç‰‡"å’¨è¯¢å¸ˆ
**ç±»å‹**ï¼šé»‘å®¢æ¾ MVPï¼ˆ48 å°æ—¶ï¼‰
**ç›®æ ‡**ï¼šå®Œæˆå¯æ¼”ç¤ºçš„å®Œæ•´äº§å“
**äº¤ä»˜æ—¥æœŸ**ï¼š______

---

## ğŸ“‹ ä»»åŠ¡æ¦‚è§ˆ

| æ¨¡å— | ä»»åŠ¡æ•° | é¢„è®¡å·¥æ—¶ | ä¼˜å…ˆçº§ | è´Ÿè´£äºº |
|------|--------|----------|--------|--------|
| å‰ç«¯å¼€å‘ | 7 | 12h | P0 | ____ |
| åç«¯å¼€å‘ | 8 | 14h | P0 | ____ |
| AI é›†æˆ | 4 | 8h | P0 | ____ |
| æ•°æ®å‡†å¤‡ | 3 | 4h | P0 | ____ |
| æµ‹è¯•è°ƒè¯• | 3 | 6h | P0 | ____ |
| æ¼”ç¤ºå‡†å¤‡ | 2 | 4h | P1 | ____ |
| **æ€»è®¡** | **27** | **48h** | - | - |

---

## ğŸ¯ Phase 1ï¼šç¯å¢ƒæ­å»ºä¸æ•°æ®å‡†å¤‡ï¼ˆ4hï¼‰

### Task 1.1ï¼šé¡¹ç›®åˆå§‹åŒ–ï¼ˆ1hï¼‰

**è´Ÿè´£äºº**ï¼š______
**ä¼˜å…ˆçº§**ï¼šP0
**ä¾èµ–**ï¼šæ— 

**å‰ç«¯**ï¼š
```bash
# åˆ›å»ºå‰ç«¯é¡¹ç›®
npm create vite@latest livestream-ai -- --template react
cd livestream-ai
npm install

# å®‰è£…ä¾èµ–
npm install hls.js socket.io-client tailwindcss
npm install -D @types/node
```

**åç«¯**ï¼š
```bash
# åˆ›å»ºåç«¯é¡¹ç›®
mkdir livestream-ai-backend
cd livestream-ai-backend
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# å®‰è£…ä¾èµ–
pip install fastapi uvicorn python-socketio
pip install sentence-transformers chromadb
pip install Pillow opencv-python
pip install openai  # Qwen API å…¼å®¹
```

**äº¤ä»˜ç‰©**ï¼š
- [ ] å‰ç«¯é¡¹ç›®å¯è¿è¡Œï¼ˆnpm run devï¼‰
- [ ] åç«¯é¡¹ç›®å¯è¿è¡Œï¼ˆéœ€è¦ä¸€ä¸ªæµ‹è¯•æ¥å£ï¼‰
- [ ] README.md è¯´æ˜å¦‚ä½•å¯åŠ¨

---

### Task 1.2ï¼šHLS è§†é¢‘å‡†å¤‡ï¼ˆ2hï¼‰

**è´Ÿè´£äºº**ï¼š______
**ä¼˜å…ˆçº§**ï¼šP0
**ä¾èµ–**ï¼šæ— 

**æ­¥éª¤**ï¼š

1. **è·å–å¸¦è´§è§†é¢‘**
   - æ‰¾ä¸€ä¸ªçœŸå®å¸¦è´§ç›´æ’­å½•åƒï¼ˆ.mp4 æ ¼å¼ï¼‰
   - æ—¶é•¿ï¼š3-5 åˆ†é’Ÿï¼ˆæ¼”ç¤ºç‰‡æ®µï¼‰
   - å†…å®¹ï¼šåŒ…å«ä¸»æ’­å±•ç¤ºå•†å“ã€è¯´æ˜ä»·æ ¼

2. **è½¬æ¢è§†é¢‘ä¸º HLS**
```bash
# å®‰è£… ffmpeg
# Windows: ä¸‹è½½ https://ffmpeg.org/download.html
# Mac: brew install ffmpeg

# è½¬æ¢å‘½ä»¤
ffmpeg -i input.mp4 \
  -c:v libx264 -c:a aac \
  -f hls \
  -hls_time 10 \
  -hls_list_size 0 \
  -start_number 0 \
  output/stream.m3u8
```

3. **å‡†å¤‡å­—å¹•æ–‡ä»¶**
   - æ‰‹åŠ¨å¬å†™æˆ–ç”¨ ASR å·¥å…·ç”Ÿæˆ
   - æ ¼å¼ï¼š
```json
[
  { "time": 10.5, "text": "æ¬¢è¿æ¥åˆ°æˆ‘çš„ç›´æ’­é—´" },
  { "time": 12.3, "text": "ä»Šå¤©ç»™å¤§å®¶å¸¦æ¥ä¸€æ¬¾è¶…å€¼å£çº¢" },
  { "time": 15.8, "text": "åŸä»·299ï¼Œç°åœ¨åªè¦99å…ƒ" },
  { "time": 20.2, "text": "è¿™æ¬¾å£çº¢æ˜¯çº¯å¤©ç„¶çš„ï¼Œå­•å¦‡å¯ç”¨" }
]
```

**äº¤ä»˜ç‰©**ï¼š
- [ ] HLS è§†é¢‘æ–‡ä»¶ï¼ˆstream.m3u8 + .ts æ–‡ä»¶ï¼‰
- [ ] å­—å¹• JSON æ–‡ä»¶ï¼ˆsubtitles.jsonï¼‰
- [ ] è§†é¢‘æ—¶é•¿è®°å½•

---

### Task 1.3ï¼šAPI Key ç”³è¯·ï¼ˆ1hï¼‰

**è´Ÿè´£äºº**ï¼š______
**ä¼˜å…ˆçº§**ï¼šP0
**ä¾èµ–**ï¼šæ— 

**æ­¥éª¤**ï¼š

1. **ç”³è¯· Qwen-Plus API**
   - è®¿é—®ï¼šhttps://dashscope.aliyuncs.com/
   - æ³¨å†Œè´¦å·
   - åˆ›å»º API Key
   - å…è´¹é¢åº¦ï¼š100 ä¸‡ tokens

2. **é…ç½®ç¯å¢ƒå˜é‡**
```bash
# åç«¯ .env æ–‡ä»¶
DASHSCOPE_API_KEY=your_api_key_here
```

3. **æµ‹è¯• API**
```python
# test_api.py
from openai import OpenAI

client = OpenAI(
    api_key=os.getenv("DASHSCOPE_API_KEY"),
    base_url="https://dashscope.aliyuncs.com/compatible-mode/v1"
)

response = client.chat.completions.create(
    model="qwen-plus",
    messages=[{"role": "user", "content": "ä½ å¥½"}]
)
print(response.choices[0].message.content)
```

**äº¤ä»˜ç‰©**ï¼š
- [ ] API Key é…ç½®åˆ° .env
- [ ] API æµ‹è¯•é€šè¿‡
- [ ] API è°ƒç”¨ç¤ºä¾‹ä»£ç 

---

## ğŸ¨ Phase 2ï¼šå‰ç«¯å¼€å‘ï¼ˆ12hï¼‰

### Task 2.1ï¼šHLS è§†é¢‘æ’­æ”¾å™¨ï¼ˆ2hï¼‰

**è´Ÿè´£äºº**ï¼š______
**ä¼˜å…ˆçº§**ï¼šP0
**ä¾èµ–**ï¼šTask 1.2

**éœ€æ±‚**ï¼š
- æ’­æ”¾ HLS è§†é¢‘
- æ˜¾ç¤ºæ’­æ”¾æ—¶é—´
- æš‚åœ/æ’­æ”¾æ§åˆ¶

**å®ç°**ï¼š
```jsx
// src/components/VideoPlayer.jsx
import { useEffect, useRef, useState } from 'react';
import Hls from 'hls.js';

export default function VideoPlayer() {
  const videoRef = useRef(null);
  const [currentTime, setCurrentTime] = useState(0);

  useEffect(() => {
    const video = videoRef.current;
    const hls = new Hls();

    hls.loadSource('/output/stream.m3u8');
    hls.attachMedia(video);

    return () => {
      hls.destroy();
    };
  }, []);

  const handleTimeUpdate = () => {
    setCurrentTime(videoRef.current.currentTime);
  };

  return (
    <div className="video-container">
      <video
        ref={videoRef}
        onTimeUpdate={handleTimeUpdate}
        controls
        width="800"
        height="450"
      />
      <div>æ’­æ”¾æ—¶é—´: {currentTime.toFixed(1)}s</div>
    </div>
  );
}
```

**äº¤ä»˜ç‰©**ï¼š
- [ ] è§†é¢‘å¯ä»¥æ’­æ”¾
- [ ] æ˜¾ç¤ºå½“å‰æ’­æ”¾æ—¶é—´
- [ ] æ ·å¼ç¾è§‚ï¼ˆç”¨ Tailwind CSSï¼‰

---

### Task 2.2ï¼šWebSocket å®¢æˆ·ç«¯ï¼ˆ2hï¼‰

**è´Ÿè´£äºº**ï¼š______
**ä¼˜å…ˆçº§**ï¼šP0
**ä¾èµ–**ï¼šTask 3.1ï¼ˆåç«¯ WebSocketï¼‰

**éœ€æ±‚**ï¼š
- è¿æ¥åç«¯ WebSocket
- å‘é€æ¨¡æ‹Ÿ ASR æ•°æ®
- å‘é€è§†é¢‘å¸§ï¼ˆOCRï¼‰
- æ¥æ”¶ AI å›ç­”

**å®ç°**ï¼š
```jsx
// src/utils/websocket.js
import { io } from 'socket.io-client';

export const socket = io('http://localhost:8000', {
  transports: ['websocket']
});

export const sendAudioData = (text, timestamp) => {
  socket.emit('audio_data', {
    text,
    timestamp,
    type: 'audio'
  });
};

export const sendVideoFrame = (imageData, timestamp) => {
  socket.emit('video_frame', {
    image: imageData,
    timestamp,
    type: 'ocr'
  });
};

export const sendQuestion = (question) => {
  socket.emit('user_question', { question });
};

export const onAnswer = (callback) => {
  socket.on('ai_answer', callback);
};
```

**äº¤ä»˜ç‰©**ï¼š
- [ ] WebSocket è¿æ¥æˆåŠŸ
- [ ] å¯ä»¥å‘é€æ•°æ®
- [ ] å¯ä»¥æ¥æ”¶å›ç­”
- [ ] é”™è¯¯å¤„ç†

---

### Task 2.3ï¼šæ•°æ®é‡‡é›†å™¨ï¼ˆ3hï¼‰

**è´Ÿè´£äºº**ï¼š______
**ä¼˜å…ˆçº§**ï¼šP0
**ä¾èµ–**ï¼šTask 2.1, Task 1.2

**éœ€æ±‚**ï¼š
- æ ¹æ®è§†é¢‘æ—¶é—´æˆ³å‘é€å­—å¹•æ•°æ®
- å®šæ—¶æˆªå›¾å¹¶å‘é€ï¼ˆæ¨¡æ‹Ÿ OCRï¼‰
- æ§åˆ¶å°æ—¥å¿—æ˜¾ç¤º

**å®ç°**ï¼š
```jsx
// src/components/DataCollector.jsx
import { useEffect, useRef } from 'react';
import { sendAudioData, sendVideoFrame } from '../utils/websocket';
import subtitles from '../data/subtitles.json';

export default function DataCollector({ videoRef }) {
  const lastSentIndex = useRef(-1);
  const lastOcrTime = useRef(0);

  useEffect(() => {
    const interval = setInterval(() => {
      if (!videoRef.current) return;

      const currentTime = videoRef.current.currentTime;

      // 1. å‘é€å­—å¹•æ•°æ®ï¼ˆæ¨¡æ‹Ÿ ASRï¼‰
      const subtitleIndex = subtitles.findIndex(
        (s, i) =>
          i > lastSentIndex.current &&
          Math.abs(s.time - currentTime) < 0.5
      );

      if (subtitleIndex !== -1) {
        const subtitle = subtitles[subtitleIndex];
        sendAudioData(subtitle.text, subtitle.time);
        console.log(`[${subtitle.time.toFixed(1)}] å‘é€è¯­éŸ³æ•°æ®:`, subtitle.text);
        lastSentIndex.current = subtitleIndex;
      }

      // 2. å‘é€è§†é¢‘å¸§ï¼ˆæ¨¡æ‹Ÿ OCRï¼‰
      if (currentTime - lastOcrTime.current > 5) {
        captureFrame(currentTime);
        lastOcrTime.current = currentTime;
      }
    }, 100);

    return () => clearInterval(interval);
  }, [videoRef]);

  const captureFrame = (currentTime) => {
    const video = videoRef.current;
    const canvas = document.createElement('canvas');
    canvas.width = video.videoWidth;
    canvas.height = video.videoHeight;
    const ctx = canvas.getContext('2d');
    ctx.drawImage(video, 0, 0);

    const imageData = canvas.toDataURL('image/jpeg', 0.7);
    sendVideoFrame(imageData, currentTime);
    console.log(`[${currentTime.toFixed(1)}] å‘é€ OCR å¸§`);
  };

  return null; // ä¸æ¸²æŸ“ä»»ä½• UI
}
```

**äº¤ä»˜ç‰©**ï¼š
- [ ] å­—å¹•æ•°æ®æŒ‰æ—¶é—´æˆ³å‘é€
- [ ] è§†é¢‘å¸§æ¯ 5 ç§’å‘é€ä¸€æ¬¡
- [ ] æ§åˆ¶å°æ˜¾ç¤ºå‘é€æ—¥å¿—
- [ ] ä¸è§†é¢‘æ’­æ”¾åŒæ­¥

---

### Task 2.4ï¼šé—®ç­”ç•Œé¢ï¼ˆ2hï¼‰

**è´Ÿè´£äºº**ï¼š______
**ä¼˜å…ˆçº§**ï¼šP0
**ä¾èµ–**ï¼šTask 2.2

**éœ€æ±‚**ï¼š
- è¾“å…¥æ¡†ï¼ˆç”¨æˆ·æé—®ï¼‰
- å†å²è®°å½•å±•ç¤º
- AI å›ç­”å±•ç¤º
- è¯æ®æ¥æºå±•ç¤º

**å®ç°**ï¼š
```jsx
// src/components/ChatPanel.jsx
import { useState, useEffect } from 'react';
import { sendQuestion, onAnswer } from '../utils/websocket';

export default function ChatPanel() {
  const [messages, setMessages] = useState([]);
  const [input, setInput] = useState('');

  useEffect(() => {
    onAnswer((data) => {
      setMessages((prev) => [...prev, { role: 'ai', ...data }]);
    });
  }, []);

  const handleSend = () => {
    if (!input.trim()) return;

    setMessages((prev) => [...prev, { role: 'user', content: input }]);
    sendQuestion(input);
    setInput('');
  };

  return (
    <div className="chat-panel">
      <div className="messages">
        {messages.map((msg, i) => (
          <div key={i} className={`message ${msg.role}`}>
            {msg.role === 'user' ? (
              <p>{msg.content}</p>
            ) : (
              <div>
                <p>{msg.answer}</p>
                {msg.evidence && (
                  <div className="evidence">
                    <h4>è¯æ®æ¥æºï¼š</h4>
                    {msg.evidence.map((ev, j) => (
                      <div key={j}>
                        [{ev.timestamp.toFixed(1)}s] {ev.text}
                      </div>
                    ))}
                  </div>
                )}
              </div>
            )}
          </div>
        ))}
      </div>

      <div className="input-area">
        <input
          type="text"
          value={input}
          onChange={(e) => setInput(e.target.value)}
          onKeyPress={(e) => e.key === 'Enter' && handleSend()}
          placeholder="é—®æˆ‘å…³äºç›´æ’­çš„é—®é¢˜..."
        />
        <button onClick={handleSend}>å‘é€</button>
      </div>
    </div>
  );
}
```

**äº¤ä»˜ç‰©**ï¼š
- [ ] å¯ä»¥è¾“å…¥é—®é¢˜
- [ ] æ˜¾ç¤º AI å›ç­”
- [ ] æ˜¾ç¤ºè¯æ®æ¥æº
- [ ] ç¾è§‚çš„ UI

---

### Task 2.5ï¼šä¸»ç•Œé¢æ•´åˆï¼ˆ2hï¼‰

**è´Ÿè´£äºº**ï¼š______
**ä¼˜å…ˆçº§**ï¼šP0
**ä¾èµ–**ï¼šTask 2.1, Task 2.3, Task 2.4

**éœ€æ±‚**ï¼š
- æ•´åˆæ‰€æœ‰ç»„ä»¶
- å“åº”å¼å¸ƒå±€
- æ¼”ç¤ºæ¨¡å¼ä¼˜åŒ–

**å®ç°**ï¼š
```jsx
// src/App.jsx
import VideoPlayer from './components/VideoPlayer';
import DataCollector from './components/DataCollector';
import ChatPanel from './components/ChatPanel';
import { useRef } from 'react';

function App() {
  const videoRef = useRef(null);

  return (
    <div className="app">
      <header>
        <h1>ç›´æ’­é—´"åˆ‡ç‰‡"å’¨è¯¢å¸ˆ</h1>
        <p>å®æ—¶è®°å¿† Â· æ™ºèƒ½é—®ç­” Â· å¤šæ¨¡æ€æ£€ç´¢</p>
      </header>

      <main>
        <div className="video-section">
          <VideoPlayer ref={videoRef} />
          <DataCollector videoRef={videoRef} />
        </div>

        <div className="chat-section">
          <ChatPanel />
        </div>
      </main>

      <footer>
        <p>æŠ€æœ¯æ ˆï¼šHLS + FastAPI + Chroma + Qwen-Plus</p>
      </footer>
    </div>
  );
}

export default App;
```

**äº¤ä»˜ç‰©**ï¼š
- [ ] å®Œæ•´çš„ç•Œé¢
- [ ] å“åº”å¼å¸ƒå±€
- [ ] æ¼”ç¤ºæ¨¡å¼ï¼ˆå…¨å±ï¼‰

---

### Task 2.6ï¼šå‰ç«¯æ ·å¼ä¼˜åŒ–ï¼ˆ1hï¼‰

**è´Ÿè´£äºº**ï¼š______
**ä¼˜å…ˆçº§**ï¼šP1
**ä¾èµ–**ï¼šTask 2.5

**éœ€æ±‚**ï¼š
- ä½¿ç”¨ Tailwind CSS
- ç»Ÿä¸€é…è‰²æ–¹æ¡ˆ
- æ¼”ç¤ºå‹å¥½çš„å¤§å­—ä½“

**äº¤ä»˜ç‰©**ï¼š
- [ ] å…¨å±€æ ·å¼é…ç½®
- [ ] ç»„ä»¶æ ·å¼
- [ ] æ¼”ç¤ºæ¨¡å¼æ ·å¼

---

## ğŸ”§ Phase 3ï¼šåç«¯å¼€å‘ï¼ˆ14hï¼‰

### Task 3.1ï¼šFastAPI + WebSocketï¼ˆ2hï¼‰

**è´Ÿè´£äºº**ï¼š______
**ä¼˜å…ˆçº§**ï¼šP0
**ä¾èµ–**ï¼šTask 1.1

**éœ€æ±‚**ï¼š
- FastAPI åŸºç¡€æ¡†æ¶
- WebSocket è¿æ¥ç®¡ç†
- CORS é…ç½®

**å®ç°**ï¼š
```python
# main.py
from fastapi import FastAPI, WebSocket, WebSocketDisconnect
from fastapi.middleware.cors import CORSMiddleware
from socketio import AsyncServer
import socketio

# FastAPI app
app = FastAPI(title="ç›´æ’­é—´åˆ‡ç‰‡å’¨è¯¢å¸ˆ API")

# CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Socket.IO
sio = AsyncServer(async_mode='asgi', cors_allowed_origins='*')
socket_app = socketio.ASGIApp(sio, app)

# è¿æ¥ç®¡ç†
connected_clients = []

@app.websocket("/ws")
async def websocket_endpoint(websocket: WebSocket):
    await websocket.accept()
    connected_clients.append(websocket)
    try:
        while True:
            data = await websocket.receive_json()
            await handle_websocket_message(websocket, data)
    except WebSocketDisconnect:
        connected_clients.remove(websocket)

async def handle_websocket_message(websocket: WebSocket, data: dict):
    """å¤„ç† WebSocket æ¶ˆæ¯"""
    message_type = data.get('type')

    if message_type == 'audio_data':
        await process_audio_data(websocket, data)
    elif message_type == 'video_frame':
        await process_video_frame(websocket, data)
    elif message_type == 'user_question':
        await process_user_question(websocket, data)

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

**äº¤ä»˜ç‰©**ï¼š
- [ ] FastAPI æœåŠ¡å™¨å¯è¿è¡Œ
- [ ] WebSocket è¿æ¥æˆåŠŸ
- [ ] åŸºç¡€è·¯ç”±é…ç½®

---

### Task 3.2ï¼šæ•°æ®æ¨¡å‹å®šä¹‰ï¼ˆ1hï¼‰

**è´Ÿè´£äºº**ï¼š______
**ä¼˜å…ˆçº§**ï¼šP0
**ä¾èµ–**ï¼šTask 3.1

**éœ€æ±‚**ï¼š
- Pydantic æ¨¡å‹
- æ•°æ®ç»“æ„å®šä¹‰

**å®ç°**ï¼š
```python
# models.py
from pydantic import BaseModel
from typing import List, Literal, Optional

class StreamData(BaseModel):
    """æµæ•°æ®æ¨¡å‹"""
    text: str
    type: Literal['audio', 'ocr']
    timestamp: float
    vector: Optional[List[float]] = None

class QuestionRequest(BaseModel):
    """ç”¨æˆ·æé—®"""
    question: str

class AnswerResponse(BaseModel):
    """AI å›ç­”"""
    question: str
    answer: str
    evidence: List[StreamData]
```

**äº¤ä»˜ç‰©**ï¼š
- [ ] æ‰€æœ‰æ•°æ®æ¨¡å‹å®šä¹‰
- [ ] ç±»å‹æ£€æŸ¥é€šè¿‡

---

### Task 3.3ï¼šEmbedding æ¨¡å‹é›†æˆï¼ˆ2hï¼‰

**è´Ÿè´£äºº**ï¼š______
**ä¼˜å…ˆçº§**ï¼šP0
**ä¾èµ–**ï¼šTask 3.2

**éœ€æ±‚**ï¼š
- åŠ è½½ sentence-transformers
- æ–‡æœ¬è½¬å‘é‡
- é¢„ç´¢å¼•

**å®ç°**ï¼š
```python
# embeddings.py
from sentence_transformers import SentenceTransformer
from typing import List
import numpy as np

# åˆå§‹åŒ–æ¨¡å‹
model = SentenceTransformer(
    'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'
)

def encode_text(text: str) -> List[float]:
    """å°†æ–‡æœ¬è½¬å‘é‡"""
    embedding = model.encode(text)
    return embedding.tolist()

def encode_texts(texts: List[str]) -> List[List[float]]:
    """æ‰¹é‡è½¬å‘é‡"""
    embeddings = model.encode(texts)
    return embeddings.tolist()

# æµ‹è¯•
if __name__ == "__main__":
    result = encode_text("è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•")
    print(f"å‘é‡ç»´åº¦: {len(result)}")
    print(f"å‰5ä¸ªå€¼: {result[:5]}")
```

**äº¤ä»˜ç‰©**ï¼š
- [ ] æ¨¡å‹åŠ è½½æˆåŠŸ
- [ ] è½¬å‘é‡åŠŸèƒ½æ­£å¸¸
- [ ] æ€§èƒ½æµ‹è¯•ï¼ˆ100 æ¡æ–‡æœ¬ < 1 ç§’ï¼‰

---

### Task 3.4ï¼šChroma å‘é‡æ•°æ®åº“ï¼ˆ2hï¼‰

**è´Ÿè´£äºº**ï¼š______
**ä¼˜å…ˆçº§**ï¼šP0
**ä¾èµ–**ï¼šTask 3.3

**éœ€æ±‚**ï¼š
- åˆå§‹åŒ– Chroma
- å­˜å‚¨å‘é‡
- æ£€ç´¢åŠŸèƒ½

**å®ç°**ï¼š
```python
# vector_store.py
import chromadb
from chromadb.config import Settings
from typing import List, Dict, Any

# åˆå§‹åŒ– Chroma
chroma_client = chroma.Client(Settings(
    anonymized_telemetry=False
))

collection = chroma_client.create_collection(
    name="livestream_memory",
    metadata={"hnsw:space": "cosine"}
)

def add_stream_data(data: StreamData):
    """æ·»åŠ æµæ•°æ®åˆ°å‘é‡æ•°æ®åº“"""
    collection.add(
        embeddings=[data.vector],
        documents=[data.text],
        metadatas=[{
            "type": data.type,
            "timestamp": data.timestamp
        }],
        ids=[f"{data.type}_{data.timestamp}"]
    )

def search_similar(query_vector: List[float], top_k: int = 3):
    """å‘é‡æ£€ç´¢"""
    results = collection.query(
        query_embeddings=[query_vector],
        n_results=top_k
    )
    return results

# æµ‹è¯•
if __name__ == "__main__":
    # æµ‹è¯•æ·»åŠ 
    test_data = StreamData(
        text="è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•",
        type="audio",
        timestamp=10.5,
        vector=[0.1] * 384  # 384 æ˜¯å‘é‡ç»´åº¦
    )
    add_stream_data(test_data)
    print("æ•°æ®æ·»åŠ æˆåŠŸ")

    # æµ‹è¯•æ£€ç´¢
    results = search_similar([0.1] * 384)
    print(f"æ£€ç´¢ç»“æœ: {results}")
```

**äº¤ä»˜ç‰©**ï¼š
- [ ] Chroma åˆå§‹åŒ–æˆåŠŸ
- [ ] å¯ä»¥æ·»åŠ æ•°æ®
- [ ] å¯ä»¥æ£€ç´¢æ•°æ®
- [ ] æ€§èƒ½æµ‹è¯•ï¼ˆæ£€ç´¢ < 100msï¼‰

---

### Task 3.5ï¼šæ•°æ®å¤„ç†æµç¨‹ï¼ˆ2hï¼‰

**è´Ÿè´£äºº**ï¼š______
**ä¼˜å…ˆçº§**ï¼šP0
**ä¾èµ–**ï¼šTask 3.3, Task 3.4

**éœ€æ±‚**ï¼š
- æ¥æ”¶éŸ³é¢‘æ•°æ® â†’ è½¬å‘é‡ â†’ å­˜å‚¨
- æ¥æ”¶è§†é¢‘å¸§ â†’ OCR â†’ è½¬å‘é‡ â†’ å­˜å‚¨
- å†…å­˜ç¼“å­˜

**å®ç°**ï¼š
```python
# processor.py
from embeddings import encode_text
from vector_store import add_stream_data
from models import StreamData
from typing import List
import base64
from PIL import Image
import io

# å†…å­˜å­˜å‚¨
memory_store: List[StreamData] = []

async def process_audio_data(websocket: WebSocket, data: dict):
    """å¤„ç†éŸ³é¢‘æ•°æ®"""
    text = data.get('text')
    timestamp = data.get('timestamp')

    # 1. è½¬å‘é‡
    vector = encode_text(text)

    # 2. æ„é€ æ•°æ®æ¨¡å‹
    stream_data = StreamData(
        text=text,
        type='audio',
        timestamp=timestamp,
        vector=vector
    )

    # 3. å­˜å‚¨åˆ°å†…å­˜
    memory_store.append(stream_data)

    # 4. å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“
    add_stream_data(stream_data)

    # 5. æ—¥å¿—
    print(f"[{timestamp}] æ”¶åˆ°è¯­éŸ³: {text}")

async def process_video_frame(websocket: WebSocket, data: dict):
    """å¤„ç†è§†é¢‘å¸§ï¼ˆOCRï¼‰"""
    image_data = data.get('image')
    timestamp = data.get('timestamp')

    # 1. è§£ç å›¾ç‰‡
    image_bytes = base64.b64decode(image_data.split(',')[1])
    image = Image.open(io.BytesIO(image_bytes))

    # 2. OCR è¯†åˆ«ï¼ˆè°ƒç”¨ Qwen-VLï¼‰
    text = await ocr_with_qwen(image)

    if not text:
        return  # æ²¡æœ‰è¯†åˆ«åˆ°æ–‡å­—

    # 3. è½¬å‘é‡
    vector = encode_text(text)

    # 4. æ„é€ æ•°æ®æ¨¡å‹
    stream_data = StreamData(
        text=text,
        type='ocr',
        timestamp=timestamp,
        vector=vector
    )

    # 5. å­˜å‚¨
    memory_store.append(stream_data)
    add_stream_data(stream_data)

    # 6. æ—¥å¿—
    print(f"[{timestamp}] OCR è¯†åˆ«: {text}")
```

**äº¤ä»˜ç‰©**ï¼š
- [ ] éŸ³é¢‘æ•°æ®å¤„ç†æ­£å¸¸
- [ ] è§†é¢‘å¸§å¤„ç†æ­£å¸¸ï¼ˆOCR é›†æˆåï¼‰
- [ ] å†…å­˜å­˜å‚¨æ­£å¸¸
- [ ] æ§åˆ¶å°æ—¥å¿—æ¸…æ™°

---

### Task 3.6ï¼šOCR é›†æˆï¼ˆ2hï¼‰

**è´Ÿè´£äºº**ï¼š______
**ä¼˜å…ˆçº§**ï¼šP0
**ä¾èµ–**ï¼šTask 1.3, Task 3.5

**éœ€æ±‚**ï¼š
- è°ƒç”¨ Qwen-VL API
- è¯†åˆ«å›¾ç‰‡ä¸­çš„æ–‡å­—
- é”™è¯¯å¤„ç†

**å®ç°**ï¼š
```python
# ocr.py
from openai import OpenAI
import os
from PIL import Image
import base64
import io

client = OpenAI(
    api_key=os.getenv("DASHSCOPE_API_KEY"),
    base_url="https://dashscope.aliyuncs.com/compatible-mode/v1"
)

async def ocr_with_qwen(image: Image.Image) -> str:
    """ä½¿ç”¨ Qwen-VL è¿›è¡Œ OCR"""
    try:
        # è½¬æ¢ä¸º base64
        buffered = io.BytesIO()
        image.save(buffered, format="JPEG")
        img_base64 = base64.b64encode(buffered.getvalue()).decode()

        # è°ƒç”¨ API
        response = client.chat.completions.create(
            model="qwen-vl-plus",
            messages=[{
                "role": "user",
                "content": [
                    {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{img_base64}"}},
                    {"type": "text", "text": "è¯·è¯†åˆ«å›¾ç‰‡ä¸­çš„æ‰€æœ‰æ–‡å­—ï¼Œåªè¾“å‡ºæ–‡å­—å†…å®¹ï¼Œä¸è¦è§£é‡Šã€‚"}
                ]
            }]
        )

        text = response.choices[0].message.content.strip()
        return text

    except Exception as e:
        print(f"OCR é”™è¯¯: {e}")
        return ""
```

**äº¤ä»˜ç‰©**ï¼š
- [ ] OCR åŠŸèƒ½æ­£å¸¸
- [ ] é”™è¯¯å¤„ç†å®Œå–„
- [ ] æ€§èƒ½æµ‹è¯•ï¼ˆå•æ¬¡ OCR < 3 ç§’ï¼‰

---

### Task 3.7ï¼šæ£€ç´¢å¼•æ“ï¼ˆ2hï¼‰

**è´Ÿè´£äºº**ï¼š______
**ä¼˜å…ˆçº§**ï¼šP0
**ä¾èµ–**ï¼šTask 3.4, Task 3.5

**éœ€æ±‚**ï¼š
- å‘é‡æ£€ç´¢
- å…³é”®è¯åŒ¹é…
- æ··åˆæ£€ç´¢

**å®ç°**ï¼š
```python
# retriever.py
from embeddings import encode_text
from vector_store import search_similar
from processor import memory_store

def extract_keywords(text: str) -> List[str]:
    """ç®€å•çš„å…³é”®è¯æå–"""
    import re
    # ç§»é™¤æ ‡ç‚¹ç¬¦å·ï¼Œåˆ†è¯
    words = re.findall(r'\w+', text)
    return words

def keyword_search(query: str, top_k: int = 3) -> List[StreamData]:
    """å…³é”®è¯æœç´¢"""
    keywords = extract_keywords(query)

    results = []
    for item in memory_store:
        # æ£€æŸ¥æ˜¯å¦åŒ…å«æ‰€æœ‰å…³é”®è¯
        if all(kw in item['text'] for kw in keywords):
            results.append(item)

    return results[:top_k]

def hybrid_search(query: str, top_k: int = 3) -> List[StreamData]:
    """æ··åˆæ£€ç´¢"""
    # 1. å‘é‡æ£€ç´¢
    query_vector = encode_text(query)
    semantic_results = search_similar(query_vector, top_k)

    # 2. å…³é”®è¯æœç´¢
    keyword_results = keyword_search(query, top_k)

    # 3. åˆå¹¶å»é‡
    all_results = semantic_results + keyword_results
    seen = set()
    unique_results = []

    for item in all_results:
        if item['timestamp'] not in seen:
            unique_results.append(item)
            seen.add(item['timestamp'])

    return unique_results[:top_k]
```

**äº¤ä»˜ç‰©**ï¼š
- [ ] å‘é‡æ£€ç´¢æ­£å¸¸
- [ ] å…³é”®è¯æœç´¢æ­£å¸¸
- [ ] æ··åˆæ£€ç´¢æ­£å¸¸
- [ ] æ€§èƒ½æµ‹è¯•ï¼ˆæ£€ç´¢ < 200msï¼‰

---

### Task 3.8ï¼šLLM é—®ç­”ï¼ˆ3hï¼‰

**è´Ÿè´£äºº**ï¼š______
**ä¼˜å…ˆçº§**ï¼šP0
**ä¾èµ–**ï¼šTask 3.7

**éœ€æ±‚**ï¼š
- æ£€ç´¢ç›¸å…³ç‰‡æ®µ
- æ„é€  Prompt
- è°ƒç”¨ Qwen-Plus
- æ ¼å¼åŒ–å›ç­”

**å®ç°**ï¼š
```python
# qa.py
from openai import OpenAI
import os
from retriever import hybrid_search
from models import AnswerResponse

client = OpenAI(
    api_key=os.getenv("DASHSCOPE_API_KEY"),
    base_url="https://dashscope.aliyuncs.com/compatible-mode/v1"
)

async def answer_question(question: str) -> AnswerResponse:
    """å›ç­”ç”¨æˆ·é—®é¢˜"""

    # 1. æ£€ç´¢
    search_results = hybrid_search(question, top_k=3)

    if not search_results:
        return AnswerResponse(
            question=question,
            answer="æŠ±æ­‰ï¼Œæˆ‘æ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚",
            evidence=[]
        )

    # 2. æ„é€  Prompt
    context = "\n".join([
        f"- [{item['timestamp']:.1f}s] {item['text']}"
        for item in search_results
    ])

    prompt = f"""
ä½ æ˜¯ä¸€ä¸ªç›´æ’­é—´åŠ©æ‰‹ï¼Œæ ¹æ®ä»¥ä¸‹ç›´æ’­ç‰‡æ®µå›ç­”ç”¨æˆ·é—®é¢˜ã€‚

ç”¨æˆ·é—®é¢˜ï¼š{question}

ç›´æ’­ç‰‡æ®µï¼š
{context}

è¯·ç”Ÿæˆä¸€ä¸ªè‡ªç„¶ã€å‹å¥½çš„å›ç­”ã€‚ç›´æ¥ç»™å‡ºç­”æ¡ˆï¼Œä¸è¦é‡å¤é—®é¢˜ã€‚
"""

    # 3. è°ƒç”¨ LLM
    try:
        response = client.chat.completions.create(
            model="qwen-plus",
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªç›´æ’­é—´åŠ©æ‰‹ã€‚"},
                {"role": "user", "content": prompt}
            ],
            temperature=0.7
        )

        answer = response.choices[0].message.content.strip()

        # 4. æ ¼å¼åŒ–
        return AnswerResponse(
            question=question,
            answer=answer,
            evidence=search_results
        )

    except Exception as e:
        print(f"LLM é”™è¯¯: {e}")
        return AnswerResponse(
            question=question,
            answer=f"æŠ±æ­‰ï¼Œå›ç­”é—®é¢˜æ—¶å‡ºé”™äº†: {str(e)}",
            evidence=search_results
        )

# å¤„ç†ç”¨æˆ·æé—®
async def process_user_question(websocket: WebSocket, data: dict):
    """å¤„ç†ç”¨æˆ·æé—®"""
    question = data.get('question')

    print(f"ç”¨æˆ·æé—®: {question}")

    # ç”Ÿæˆå›ç­”
    response = await answer_question(question)

    # å‘é€ç»™å‰ç«¯
    await websocket.send_json({
        "type": "ai_answer",
        "data": response.dict()
    })

    print(f"AI å›ç­”: {response.answer}")
```

**äº¤ä»˜ç‰©**ï¼š
- [ ] é—®ç­”åŠŸèƒ½æ­£å¸¸
- [ ] å›ç­”è´¨é‡æµ‹è¯•
- [ ] æ€§èƒ½æµ‹è¯•ï¼ˆç«¯åˆ°ç«¯ < 2 ç§’ï¼‰

---

## ğŸ§ª Phase 4ï¼šæµ‹è¯•ä¸è°ƒè¯•ï¼ˆ6hï¼‰

### Task 4.1ï¼šå•å…ƒæµ‹è¯•ï¼ˆ2hï¼‰

**è´Ÿè´£äºº**ï¼š______
**ä¼˜å…ˆçº§**ï¼šP1
**ä¾èµ–**ï¼šæ‰€æœ‰å¼€å‘ä»»åŠ¡

**æµ‹è¯•é¡¹**ï¼š
- [ ] Embedding æ¨¡å‹æµ‹è¯•
- [ ] å‘é‡æ£€ç´¢æµ‹è¯•
- [ ] OCR åŠŸèƒ½æµ‹è¯•
- [ ] LLM é—®ç­”æµ‹è¯•

**äº¤ä»˜ç‰©**ï¼š
- [ ] æµ‹è¯•æŠ¥å‘Š
- [ ] Bug åˆ—è¡¨

---

### Task 4.2ï¼šé›†æˆæµ‹è¯•ï¼ˆ2hï¼‰

**è´Ÿè´£äºº**ï¼š______
**ä¼˜å…ˆçº§**ï¼šP0
**ä¾èµ–**ï¼šTask 4.1

**æµ‹è¯•æµç¨‹**ï¼š
1. å¯åŠ¨åç«¯
2. å¯åŠ¨å‰ç«¯
3. æ’­æ”¾è§†é¢‘
4. è§‚å¯Ÿæ•°æ®æµ
5. æé—®æµ‹è¯•

**äº¤ä»˜ç‰©**ï¼š
- [ ] ç«¯åˆ°ç«¯æµ‹è¯•é€šè¿‡
- [ ] Bug ä¿®å¤

---

### Task 4.3ï¼šæ€§èƒ½ä¼˜åŒ–ï¼ˆ2hï¼‰

**è´Ÿè´£äºº**ï¼š______
**ä¼˜å…ˆçº§**ï¼šP1
**ä¾èµ–**ï¼šTask 4.2

**ä¼˜åŒ–é¡¹**ï¼š
- [ ] å‘é‡æ£€ç´¢é€Ÿåº¦ï¼ˆ< 200msï¼‰
- [ ] OCR å¹¶å‘æ§åˆ¶
- [ ] LLM è°ƒç”¨ç¼“å­˜
- [ ] WebSocket æ¶ˆæ¯é˜Ÿåˆ—

**äº¤ä»˜ç‰©**ï¼š
- [ ] æ€§èƒ½æµ‹è¯•æŠ¥å‘Š
- [ ] ä¼˜åŒ–åæ€§èƒ½æŒ‡æ ‡

---

## ğŸ¬ Phase 5ï¼šæ¼”ç¤ºå‡†å¤‡ï¼ˆ4hï¼‰

### Task 5.1ï¼šæ¼”ç¤ºæ•°æ®å‡†å¤‡ï¼ˆ2hï¼‰

**è´Ÿè´£äºº**ï¼š______
**ä¼˜å…ˆçº§**ï¼šP0
**ä¾èµ–**ï¼šæ‰€æœ‰å¼€å‘ä»»åŠ¡

**ä»»åŠ¡**ï¼š
1. å‡†å¤‡ 3 ä¸ªé—®ç­”æ¡ˆä¾‹
2. æµ‹è¯•æ¯ä¸ªæ¡ˆä¾‹
3. å‡†å¤‡å¤‡ç”¨æ–¹æ¡ˆ

**äº¤ä»˜ç‰©**ï¼š
- [ ] æ¡ˆä¾‹æµ‹è¯•é€šè¿‡
- [ ] æ¼”ç¤ºè„šæœ¬

---

### Task 5.2ï¼šæ¼”ç¤ºæ¼”ç»ƒï¼ˆ2hï¼‰

**è´Ÿè´£äºº**ï¼š______
**ä¼˜å…ˆçº§**ï¼šP0
**ä¾èµ–**ï¼šTask 5.1

**ä»»åŠ¡**ï¼š
1. å®Œæ•´æ¼”ç¤ºæµç¨‹ï¼ˆ3-5 åˆ†é’Ÿï¼‰
2. æ—¶é—´æ§åˆ¶
3. å¤‡ç”¨æ–¹æ¡ˆæµ‹è¯•

**äº¤ä»˜ç‰©**ï¼š
- [ ] æ¼”ç¤ºè§†é¢‘å½•åˆ¶
- [ ] æ¼”è®²ç¨¿

---

## ğŸ“Š è¿›åº¦è·Ÿè¸ª

### æ¯æ—¥ç«™ä¼šï¼ˆå»ºè®®ï¼‰

**æ¯å¤©ä¸Šåˆ 10:00ï¼Œ15 åˆ†é’Ÿ**

**æ¯ä¸ªæˆå‘˜æ±‡æŠ¥**ï¼š
1. æ˜¨å¤©å®Œæˆäº†ä»€ä¹ˆ
2. ä»Šå¤©è®¡åˆ’åšä»€ä¹ˆ
3. é‡åˆ°ä»€ä¹ˆé˜»ç¢

### ä»»åŠ¡çœ‹æ¿

ä½¿ç”¨ GitHub Projects æˆ– Trelloï¼š

```
[Backlog] â†’ [To Do] â†’ [In Progress] â†’ [Done]
```

---

## âœ… éªŒæ”¶æ ‡å‡†

### æŠ€æœ¯éªŒæ”¶
- [ ] HLS è§†é¢‘æ’­æ”¾æµç•…
- [ ] æ•°æ®é‡‡é›†å®æ—¶åŒæ­¥ï¼ˆ< 100ms å»¶è¿Ÿï¼‰
- [ ] å‘é‡æ£€ç´¢å‡†ç¡®ç‡ > 80%
- [ ] OCR è¯†åˆ«ç‡ > 70%
- [ ] LLM å›ç­”è´¨é‡ > 4/5

### æ¼”ç¤ºéªŒæ”¶
- [ ] 3-5 åˆ†é’Ÿå®Œæ•´æ¼”ç¤º
- [ ] è‡³å°‘ 2 ä¸ªé—®ç­”æ¡ˆä¾‹
- [ ] æ— é‡å¤§ Bug
- [ ] è¯„å§”èƒ½ç†è§£æ ¸å¿ƒä»·å€¼

---

## ğŸš¨ é£é™©ä¸åº”å¯¹

| é£é™© | æ¦‚ç‡ | å½±å“ | åº”å¯¹ |
|------|------|------|------|
| OCR æˆæœ¬è¶…æ”¯ | ä¸­ | ä¸­ | é™åˆ¶ OCR é¢‘ç‡ï¼Œä½¿ç”¨å‡æ•°æ® |
| LLM å“åº”æ…¢ | ä¸­ | ä¸­ | å±•ç¤ºåŠ è½½åŠ¨ç”»ï¼Œä½¿ç”¨ç¼“å­˜ |
| æ¼”ç¤ºæ—¶ç½‘ç»œé—®é¢˜ | é«˜ | é«˜ | å‡†å¤‡ç¦»çº¿è§†é¢‘ï¼Œæœ¬åœ°æœåŠ¡å™¨ |
| API è¾¾åˆ°é™é¢ | ä½ | ä¸­ | å‡†å¤‡å¤‡ç”¨ API Key |

---

## ğŸ“ è”ç³»æ–¹å¼

**æŠ€æœ¯è´Ÿè´£äºº**ï¼š______
**å‰ç«¯è´Ÿè´£äºº**ï¼š______
**åç«¯è´Ÿè´£äºº**ï¼š______

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0
**æœ€åæ›´æ–°**: 2025-12-31
**é¢„è®¡å®Œæˆ**: ____
